---
layout: post
current: post
cover: /assets/images/saliency/saliency-cover.png
navigation: True
title: "You Can't Understand A Deep Learning Model by Watching Where It's Looking"
date: 2021-01-24 13:32:20 +0300
tags: [Publications]
class: post-template
subclass: 'post'
author: josephdviviano
---

Deep learning's promise in medical applications are obvious, but it will be hard to get wide acceptance (or regulatory approvals) without doctors being able to understand the "why" behind each prediction. A popular method for doing so is looking at a model's "saliency" map, which looks like a weather map on top of the image used to make the prediction. The brightest areas are believed to be the regions most involved in the prediction.

This method led to some concern. Take this example, from [this excellent paper by John R. Zech et al.](https://arxiv.org/abs/1807.00431). In it, we can see that the model is using a metal token that radiology technicians place on the patient (top right hand corner) during image capture to make the prediction (the token helps Radiologists distinguish left from right later on). The issue is when the model notices that people who have a particular kind of token are also more likely to have some disease, say pneumonia, or cardiomegaly (enlarged heart), the model will seem to use the token when making the prediction.

{:refdef: style="text-align: center;"}
![Bad Saliency]({{site.baseurl}}/assets/images/saliency/bad-saliency.png)
{: refdef}

These kinds of relationships are found everywhere, and called "spurious correlations". The problem with spurious correlations is you can't rely one them... they might be very high in one dataset, but very low in another, because there is no reason for the two variables to be related at all, and the original correlation was just due to bad luck. Humans are generally good at detecting and discarding these kinds of correlations when they are simple, but very bad at discarding them when the real relationships aren't immediately apparent. For example, most people would discard this correlation as spurious:

{:refdef: style="text-align: center;"}
![Bad Correlation]({{site.baseurl}}/assets/images/saliency/bad-correlation.png)
{: refdef}

but people often fall for thought leaders who previously predicted some impossible-to-predict event, ignoring the fact that thousands of people were making wrong predictions at the same time and it is possible that this thought leader might have just been lucky. Very common in finance.

This project was inspired by the above problem. We know where doctors want the models to look for disease, and we can see where the model is looking, so maybe we can directly teach the model to look in the right place, and show that this helps the model generalize to new situations better.

I'm ecstatic that my first publication in a "real" machine learning venue was the [International Conference on Learning Representations (ICLR)](https://iclr.cc/). I'll summarize the [paper](https://openreview.net/forum?id=c9-WeM-ceB) soon.