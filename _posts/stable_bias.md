---
layout: post
current: post
cover: /assets/images/stable_bias.png
navigation: True
title: "Stable Bias: Analyzing Societal Representations in Diffusion Models"
date: 2023-06-27
tags: [Publications]
class: post-template
subclass: 'post'
author: sashaluccioni
---
As AI-enabled Text-to-Image models are becoming increasingly used, characterizing the social biases they exhibit is a necessary first step to lowering their risk of discriminatory outcomes.

We compare three such models: Stable Diffusion v.1.4, Stable Diffusion v.2., and Dall-E 2, prompting them to produce images of different professions and identity characteristics.

You can explore our findings in the sections below:

## How do Diffusion Models Represent Identity?

One of the goals of our study was to look at the ways in which pictures generated by text-to-image models depict different notions of gender and ethnicity. These concepts are inherently difficult to describe, however: gender and ethnicity are multi-dimensional, inter-related, and, most importantly, socially constructed: they cannot (and should not) be predicted based on appearance features alone. Since we are working with depictions of fictive humans when analyzing text-to-image model behaviors, we cannot rely on self-identification either to assign identity categories to individual data points. Instead, we develop a clustering-based method to identify relevant dataset-level trends in the generated images, as described in our companion [Identity Representation Demo](https://huggingface.co/spaces/society-ethics/DiffusionFaceClustering).

## Quantifying Social Biases in Image Generations: Professions

In the previous section, we provided a method to characterize how text-to-image systems associate textual mentions of identity characteristics with visual features in their outputs. Exploring the inherent representations learned through our method already gave us insights into some of the bias dynamics embedded in the models that support these systems. We take the analysis further in the present section by leveraging these represenations to study social biases tied to a particular application setting: specifically, we consider a setting where users generate pictures of people in professional settings, for example to use in place of stock images in articles or websites. To that end, we systematically generate a large variety of images for 150 professions for each of 3 systems, and compare the distribution of the visual features identified in the [Identity Representation Demo](https://hf.co/spaces/society-ethics/DiffusionFaceClustering) across all generations and across generations for each profession. You can explore these results in detail in the companion [Profession Bias Tool](https://hf.co/spaces/society-ethics/DiffusionClustering) - in particular, you can read about the resulting diversity metric for different models and professions, as well as a study case for comparing the representations of different mental health professions, by expanding the accordion below:

## Comparing Model Generations

Above and beyond quantitative analyses, one of the main goals of our project was to create accessible ways for the users to explore the generated images themselves, based on their own interests. For this purpose, we created two interactive tools: the [Diffusion Bias Explorer](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer), which can be used to compare two models and the images they generate for a given profession or for a given model across two professions, and the [Average Diffusion Faces Tool](https://huggingface.co/spaces/society-ethics/Average_diffusion_faces), which shows an 'average' representation of faces across professions, based on the images generated by the 3 models.

This work was done by Alexandra Sasha Luccioni (Hugging Face), Christopher Akiki (ScaDS.AI, Leipzig University), Margaret Mitchell (Hugging Face), and Yacine Jernite (Hugging Face).
